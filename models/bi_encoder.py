import collections
from statistics import mean

import pytorch_lightning as pl
import torch
import torch.nn as nn
from torch import optim
from torch.optim.lr_scheduler import LambdaLR
from transformers import AutoModel, AutoTokenizer
from torchmetrics import RetrievalMRR, RetrievalRecall


class BertBiEncoder(pl.LightningModule):
    def __init__(
        self,
        pretrained_model_name,
        num_classes,
        truncate,
        lr=1e-6,
        warm_up_step=10000,
    ):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(pretrained_model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(
            pretrained_model_name, use_fast=True
        )
        self.truncate = truncate
        self.loss_func = nn.MultiLabelMarginLoss()
        self.loss_func = nn.MultiMarginLoss()
        self.loss_func = nn.CrossEntropyLoss()
        self.lr = lr
        self.warm_up_step = warm_up_step

    def _mean_pooling(self, last_hidden_states, attention_mask):
        token_embeddings = last_hidden_states
        input_mask_expanded = (
            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        )
        pooled_embedding = torch.sum(
            token_embeddings * input_mask_expanded, axis=1
        ) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
        return pooled_embedding

    def compute_embeddings(self, input_text_list, token_type_id=0, mode="mean_pool"):

        if mode not in ["CLS", "mean_pool"]:
            raise ValueError("mode is {mode} but it should be [CLS, mean_pool]")

        encoded_inputs = self.tokenizer(
            input_text_list,
            return_tensors="pt",
            max_length=self.truncate,
            truncation="longest_first",
            padding="max_length",
        )
        # encoded_inputs is a dictionary with three keys: input_ids, attention_mask, and token_type_ids
        # the position ids [0, 1, ..., seq_len - 1] will be generated by default on the fly in the cross_encoder
        encoded_inputs = {k: v.to(self.device) for k, v in encoded_inputs.items()}

        if "token_type_ids" in encoded_inputs:
            encoded_inputs["token_type_ids"] = torch.full(
                encoded_inputs["token_type_ids"].size(),
                token_type_id,
                dtype=torch.long,
                device=self.device,
            )
        encoder_outputs = self.encoder(**encoded_inputs)
        # bert_output is the last layer's hidden state
        last_hidden_states = encoder_outputs.last_hidden_state

        if mode == "mean_pool":
            mean_embeddings = self._mean_pooling(
                last_hidden_states, encoded_inputs["attention_mask"]
            )
        elif mode == "CLS":
            cls_representations = last_hidden_states[:, 0, :]
            mean_embeddings = cls_representations
        # mean embeddings has shape of batch_size x hidden_dim
        return mean_embeddings

    def training_step(self, batch, batch_idx=0):

        query_text_list, doc_text_list, labels = batch
        # for query, we use token type id 0; for doc, we use token type id 1
        query_embeddings = self.compute_embeddings(query_text_list, token_type_id=0)
        doc_embeddings = self.compute_embeddings(doc_text_list, token_type_id=1)

        all_similarities = torch.matmul(query_embeddings, doc_embeddings.T)

        loss = self.loss_func(all_similarities, labels)

        self.log("loss", loss.item(), on_step=True, on_epoch=True, prog_bar=True)
        return loss

    def on_validation_epoch_start(self) -> None:

        self.query_candidates_scores = collections.defaultdict(list)
        self.query_ids = []
        self.preds = []
        self.rel_label = []

    def validation_step(self, batch, batch_idx=0):
        qd_ids, input_text_pairs, labels = batch
        query_text_list, doc_text_list = zip(*input_text_pairs)
        query_embeddings = self.model_forward(list(query_text_list), token_type_id=0)
        doc_embeddings = self.model_forward(list(doc_text_list), token_type_id=1)
        similarities = torch.sum(query_embeddings * doc_embeddings, dim=1)

        for qd_id, sim_score in zip(qd_ids, similarities):
            self.query_candidates_scores[qd_id[0]].append((qd_id[2], sim_score))
            self.query_ids.append(int(qd_id[0]))
            self.preds.append(sim_score.item())
            self.rel_label.append(qd_id[2])

    def validation_epoch_end(self, output_results):

        preds = torch.Tensor(self.preds)
        targets = torch.tensor(self.rel_label).long()
        indexes = torch.tensor(self.query_ids).long()
        recall_at_5 = RetrievalRecall(k=5)(preds, targets, indexes=indexes)
        recall_at_20 = RetrievalRecall(k=20)(preds, targets, indexes=indexes)
        recall_at_100 = RetrievalRecall(k=100)(preds, targets, indexes=indexes)
        recall_at_200 = RetrievalRecall(k=100)(preds, targets, indexes=indexes)
        recall_at_1000 = RetrievalRecall(k=1000)(preds, targets, indexes=indexes)

        mrr = RetrievalMRR()(preds, targets, indexes=indexes)

        self.log("val_recall@5", recall_at_5, prog_bar=True)
        self.log("val_recall@20", recall_at_20, prog_bar=True)
        self.log("val_recall@100", recall_at_100, prog_bar=True)
        self.log("val_recall@200", recall_at_200, prog_bar=True)
        self.log("val_recall@1000", recall_at_1000, prog_bar=True)
        self.log("val_mrr", mrr, on_epoch=True, prog_bar=True)

        return mrr

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=self.lr)
        return optimizer


if __name__ == "__main__":
    model = BertBiEncoder(
        pretrained_model_name="distilbert-base-uncased",
        num_classes=2,
        truncate=120,
    )

    import os

    from dataset.ms_marco_data import (
        MSQDPairData,
        get_data_loader,
        text_pair_collate_fn,
    )

    data_root = "/mnt/d/MLData/Repos/neural_IR/experiments/msmarco_psg_ranking/cross_encoder_triplet_train_data_tiny"
    pid_2_passages_path = os.path.join(data_root, "pid_2_passage_text.pkl")
    qid_2_query_path = os.path.join(data_root, "qid_2_query_text.pkl")
    triplet_path = os.path.join(data_root, "triplets.pkl")
    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased", use_fast=False)

    dataset = MSQDPairData(triplet_path, qid_2_query_path, pid_2_passages_path)

    data_loader = get_data_loader(
        dataset, batch_size=4, collate_fn=text_pair_collate_fn
    )

    for batch in data_loader:
        model.training_step(batch)
        break
